\section{Conclusion and further improvements}
In conclusion, \textit{our implementation of} MCTS turned out to be unefficient against minimax, even with a small parameter of search. This was confirmed by a very low win rate of 5\% on 320 games, and using MCTS's version with the largest execution time while staying realist (15-20 seconds $\Rightarrow n=20k$). This result led to the major conclusion that with the same execution time, a static version of MCTS is very poor compared to minimax. \\

Furthermore, an analysis of the average execution times of the AI throughout a game has led to conclusions presented in section \ref{subsection:benchmark-conclusion}, giving tips on how MCTS could be used more appropriately. \\

The only objective to maximize was for us the win rate, because we simply thought that correcting our MCTS implementation would be enough to win games and start optimizing the win rate over $p$. The results were pretty straight forward and the win rate was always near zero. Then, a further improvement can be made on the \textbf{benchmark}, not considering only one binary output "win or loss", but starting to take other things into account : how far has the game gone, how many pieces are left, etc., \textit{i.e} \textbf{using heuristics} to evaluate a game. \\

Using heuristics on the simulated games would allow us to rank them according to their performance so see which kind of $p$ gives good performances : high exploration or exploitation ? So, the use of heuristics to improve the benchmark would probably the first thing we would do to get better value of $p$. Careful : heuristics need to be wisely chosen, analyzed, etc. \\

Secondly, the implementation of the game could be analysed more in details to clean more code and try to get $n$ higher for realistic times. The use of more efficient arrays structures in Python (use of Numpy, for example) could lead to execution time decrease, etc. \\

Finally, we could make use of the analysis and make a dynamic AI using different algorithms according to the current state of the game : use minimax at the beginning, MCTS at the end, changing $n$ during the game, etc.  \\

This project allowed us to implement an AI algorithm and see in front of our eyes that things are not as easy as "implementing and see winning". Benchmarking allowed us to understand the behaviour of the AI, imagine how it can be better used, what approaches can be lead to improve it, and even better : what would we be more focused on if we had to go over the same work case.